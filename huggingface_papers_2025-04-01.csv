标题,中文标题,领域分类,研究机构,PDF链接,论文链接,简明摘要,Upvote数
MoCha: Towards Movie-Grade Talking Character Synthesis,MoCha：迈向电影级对话角色合成,Other,Meta,https://arxiv.org/pdf/2503.23307,https://huggingface.co/papers/2503.23307,论文《MoCha: Towards Movie-Grade Talking Character Synthesis》提出了一种新的生成模型MoCha，旨在从语音和文本直接生成逼真的谈话角色动画。与传统的“说话头”技术不同，MoCha能够生成完整的人物形象，增强了角色驱动的叙事能力。该模型采用了一种语音-视频窗口注意机制，以确保视频与语音的精准同步，并通过联合训练策略克服了大规模标注视频数据集的不足。此外，MoCha引入了结构化提示模板，首次实现了多角色的回合制对话。大量评估表明，MoCha在现实性、表现力和可控性方面显著提升，为自动化电影制作设定了新标准。,18
"What, How, Where, and How Well? A Survey on Test-Time Scaling in Large
  Language Models",什么、如何、在哪里以及效果如何？关于大语言模型测试时扩展的调查,LLM,Other,https://arxiv.org/pdf/2503.24235,https://huggingface.co/papers/2503.24235,本论文对测试时缩放（TTS）在大型语言模型中的研究进行了全面调查，提出了一个统一的多维框架，涵盖了TTS研究的四个核心维度：缩放内容、缩放方法、缩放位置和缩放效果。作者回顾了现有的方法、应用场景和评估标准，系统性地解析了各技术在TTS领域中的独特功能角色。通过分析，论文总结了TTS的发展轨迹，并提供了实际部署的指导建议。同时，识别出若干开放性挑战，并探讨了未来的研究方向，如进一步缩放、技术功能的明确化及任务的广泛适用性。,17
"RIG: Synergizing Reasoning and Imagination in End-to-End Generalist
  Policy",RIG：在端到端通用策略中协同推理与想象,Embodied AI,Shanghai AI Lab,https://arxiv.org/pdf/2503.24388,https://huggingface.co/papers/2503.24388,本文提出了一种名为RIG的端到端通用策略，首次将推理与想象相结合，以提高在复杂开放环境中嵌入式代理的表现。通过构建一个数据管道，RIG在训练过程中逐步整合推理和想象的内容，从而实现更高的学习效率和泛化能力。实验结果表明，与以往方法相比，RIG在样本效率上提升超过17倍，并且在推理、行动和环境动态之间建立了明确的关联。该方法不仅增强了策略的稳健性和可扩展性，还支持在推理阶段自我修正，提升了整体性能。,16
"TextCrafter: Accurately Rendering Multiple Texts in Complex Visual
  Scenes",TextCrafter：在复杂视觉场景中准确渲染多重文本,Diffusion Model,Other,https://arxiv.org/pdf/2503.23461,https://huggingface.co/papers/2503.23461,本论文提出了一种新的复杂视觉文本生成方法，名为TextCrafter，旨在解决在视觉图像中生成多区域文本时常见的模糊、失真和遗漏问题。TextCrafter通过逐步分解复杂文本组件，并确保文本与其视觉载体之间的良好对齐，来提高文本渲染的准确性。此外，论文引入了一种增强机制，以提高视觉文本的显著性。研究还推出了新的基准数据集CVTG-2K，用于严格评估生成模型在复杂视觉文本生成任务中的表现。实验结果表明，TextCrafter在性能上优于现有的最先进方法。,14
Effectively Controlling Reasoning Models through Thinking Intervention,通过思维干预有效控制推理模型,LLM,Other,https://arxiv.org/pdf/2503.24370,https://huggingface.co/papers/2503.24370,本论文提出了一种名为“思维干预”的新范式，旨在通过插入或修改特定的思维标记来有效控制大型语言模型（LLMs）的内部推理过程。研究表明，这种方法显著提高了模型在多个任务上的表现，包括指令遵循、指令层次推理和安全性对齐，在指令遵循场景中准确率提升达6.7%，而在不安全提示的拒绝率上提高了40%。思维干预不仅不需要模型训练，且与现有的模型控制技术兼容，为更精细的推理模型控制开辟了新的研究方向。,9
"TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud
  Detection",TeleAntiFraud-28k：用于电信欺诈检测的音频-文本慢思维数据集,Multimodal LLM,Other,https://arxiv.org/pdf/2503.24115,https://huggingface.co/papers/2503.24115,"本论文提出了TeleAntiFraud-28k，这是首个专为电信诈骗检测设计的开放源代码音频-文本慢思维数据集。该数据集通过三种策略构建，确保数据隐私并增强场景覆盖，包括使用自动语音识别生成的文本样本、基于大语言模型的语义增强以及多代理对抗合成以模拟新兴诈骗策略。数据集包含28,511对经过严格处理的语音-文本配对，涵盖场景分类、诈骗检测和诈骗类型分类等任务。此外，研究团队还构建了TeleAntiFraud-Bench评估基准，促进模型性能的系统测试。此项目为多模态反欺诈研究奠定了基础，解决了数据隐私和场景多样性等关键挑战。",7
"Classical Planning with LLM-Generated Heuristics: Challenging the State
  of the Art with Python Code",使用LLM生成启发式函数的经典规划：用Python代码挑战最先进技术,LLM,Other,https://arxiv.org/pdf/2503.18809,https://huggingface.co/papers/2503.18809,本论文探讨了利用大型语言模型（LLMs）生成启发式函数以改善经典规划的能力。研究表明，LLMs能够为特定规划领域生成有效的启发式函数，这些函数在贪婪最佳优先搜索中表现优于现有的领域无关启发式方法，甚至与最强的领域依赖学习算法相当。尽管我们的实现基于未优化的Python规划器，但LLM生成的启发式函数在某些领域扩展的状态数量更少，显示出其计算效率和信息量的优势。总体而言，这项研究展示了通过采样规划启发式函数程序，显著提升LLMs的规划能力。,6
Query and Conquer: Execution-Guided SQL Generation,查询与征服：执行引导的SQL生成,LLM,Other,https://arxiv.org/pdf/2503.24364,https://huggingface.co/papers/2503.24364,本文提出了一种新颖的SQL生成方法，通过利用执行结果来选择多个候选查询中最具语义一致性的一个，从而显著提高文本到SQL任务的准确性。这一方法使得小型、成本效益高的模型能够超越计算密集型推理方法，同时将推理成本降低至原来的30倍。该方法与现有模型无缝集成，为实现先进的SQL生成提供了实用且可扩展的解决方案。通过关注执行级别的等价性，本文有效缩小了单次准确率与多次准确率之间的差距。,3
SketchVideo: Sketch-based Video Generation and Editing,SketchVideo: 基于草图的视频生成与编辑,Other,Other,https://arxiv.org/pdf/2503.23284,https://huggingface.co/papers/2503.23284,本文提出了一种名为SketchVideo的草图基础视频生成和编辑方法，旨在解决文本和图像条件下视频生成中的布局和几何细节控制问题。该方法基于DiT视频生成模型，设计了高效的草图控制结构，通过在关键帧上绘制草图进行空间和运动控制。为实现跨帧的草图条件传播，提出了帧间注意力机制，并设计了视频插入模块以保持编辑内容与原视频的一致性。实验表明，SketchVideo在可控视频生成和编辑方面表现优越，为用户提供了更灵活的交互方式。,3
Efficient Inference for Large Reasoning Models: A Survey,大型推理模型的高效推理：综述,LLM,Other,https://arxiv.org/pdf/2503.23077,https://huggingface.co/papers/2503.23077,本论文对大型推理模型（LRMs）进行了一项全面的调查，重点关注提高推理效率的方法。尽管LRMs在复杂任务解决中表现出色，但其推理过程存在令牌使用、内存消耗和推理时间的低效问题。论文将现有方法分为两大类：显式紧凑的思维链（CoT）和隐式潜在的思维链，并分析了各自的优缺点。此外，论文探讨了人性化可控推理、可解释性与效率之间的权衡等开放挑战，并提出通过模型合并、新架构和代理路由等技术提升LRMs推理效率的关键见解，为研究人员提供了有价值的指导。,3
"KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large
  Vision-Language Models in the Korean Language",KOFFVQA：针对大型视觉语言模型的客观评估自由形式视觉问答基准（韩语）,Multimodal LLM,Other,https://arxiv.org/pdf/2503.23730,https://huggingface.co/papers/2503.23730,本文提出了KOFFVQA，一个针对大型视觉语言模型（VLMs）的自由形式视觉问答基准，专为韩语设计。该基准包含275个精心设计的问题，每个问题配有图像和涵盖VLM表现的10个评分标准，旨在解决现有评估方法的主观性和不可靠性。通过使用预设评分规则，KOFFVQA能够客观地评估模型表现，甚至允许小型开源模型进行评估。实验结果表明，采用这种基于标准化评分的评估方法比传统方法更为可靠。该基准为韩语VLMs的评估提供了必要的工具，填补了现有评估体系的空白。,2
"Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement
  Learning on the Base Model",Open-Reasoner-Zero：一种开源方法以扩展基础模型上的强化学习,LLM,THU,https://arxiv.org/pdf/2503.24290,https://huggingface.co/papers/2503.24290,本文介绍了Open-Reasoner-Zero，这是首个开源的大规模推理导向强化学习实现，旨在提升可扩展性、简洁性和可访问性。通过实验，研究表明，采用简单的PPO算法和基于规则的奖励机制，无需KL正则化，能够有效提高响应长度和基准性能。与DeepSeek-R1-Zero-Qwen-32B相同的基础模型下，Open-Reasoner-Zero在AIME2024、MATH500和GPQA Diamond基准上表现出色，训练步骤仅需DeepSeek-R1-Zero的十分之一。为推动开源研究，论文发布了源代码、参数设置、训练数据和不同规模的模型权重。,1
"Bridging Evolutionary Multiobjective Optimization and GPU Acceleration
  via Tensorization",通过张量化桥接进化多目标优化与GPU加速,Other,Other,https://arxiv.org/pdf/2503.20286,https://huggingface.co/papers/2503.20286,该论文提出了一种通过张量化方法将进化多目标优化（EMO）算法与GPU加速相结合的创新方案。随着问题规模和复杂性的增加，传统EMO算法在性能上面临挑战，而GPU的使用尚未得到充分利用。通过将EMO算法的数据结构和操作转化为紧凑的张量表示，研究者实现了在GPU上自动并行化。实验表明，张量化的EMO算法在速度上比CPU版本快达1113倍，同时保持了解决方案的质量，并能够处理复杂的多目标机器人控制任务。该方法为EMO算法的高效应用提供了新的视角和技术支持。,1
Expanding RL with Verifiable Rewards Across Diverse Domains,在多样化领域中扩展具有可验证奖励的强化学习,LLM,Tencent,https://arxiv.org/pdf/2503.23829,https://huggingface.co/papers/2503.23829,本文探讨了可验证奖励的强化学习（RLVR）在医学、化学、心理学和经济学等多样化领域的扩展应用。研究发现，当存在客观参考答案时，不同的大型语言模型（LLMs）在二元判断上高度一致，这表明无需大量标注即可训练领域特定的奖励模型。为克服处理非结构化参考答案时二元奖励的局限，研究者引入了基于模型的软评分，提升了RLVR的灵活性。实验结果显示，经过微调的生成奖励模型能够有效提供跨领域的可靠奖励信号，显著超越现有的开源对齐LLMs，展示了RLVR在实际应用中的潜力，尤其是在面对噪声或弱标签时。,0
