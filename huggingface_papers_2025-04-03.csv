标题,中文标题,领域分类,研究机构,PDF链接,论文链接,简明摘要,Upvote数
"MergeVQ: A Unified Framework for Visual Generation and Representation
  with Disentangled Token Merging and Quantization",MergeVQ：一种用于视觉生成和表示的统一框架，具有解耦的标记合并和量化,Other,Other,https://arxiv.org/pdf/2504.00999,https://huggingface.co/papers/2504.00999,本文提出了MergeVQ，一个统一的框架，旨在提升视觉生成和表示学习的效率与质量。MergeVQ通过在基于向量量化的自回归生成模型中引入令牌合并技术，解决了生成质量与表示学习之间的权衡问题。在预训练阶段，MergeVQ利用令牌合并模块解耦潜在空间中的语义，并通过交叉注意力模块恢复细节。在生成阶段，MergeAR通过键值缓存压缩实现高效的预测。实验结果表明，MergeVQ在图像生成和视觉表示学习任务中表现出色，同时保持了良好的令牌效率和推理速度。,47
Improved Visual-Spatial Reasoning via R1-Zero-Like Training,通过R1-Zero类训练改善视觉空间推理,Multimodal LLM,Other,https://arxiv.org/pdf/2504.00883,https://huggingface.co/papers/2504.00883,本论文探讨了通过R1-Zero-like训练提升多模态大型语言模型（MLLMs）的视觉空间推理能力。研究发现，Qwen2-VL模型在使用思维链提示时无法激活其视觉空间推理能力，因此引入了GRPO训练方法，并利用精心构建的VSI-100k数据集进行改进。经过120小时的GPU训练，vsGRPO-2B模型的性能超过基线模型12.1%，而vsGRPO-7B模型的表现与顶级开源模型LLaVA-NeXT-Video-72B相当。此外，vsGRPO在与监督微调和直接偏好优化基线的比较中显示出显著优势。研究成果将为视觉空间推理的进一步发展提供重要参考。,38
"AnimeGamer: Infinite Anime Life Simulation with Next Game State
  Prediction",AnimeGamer：具有下一游戏状态预测的无限动漫生活模拟,Multimodal LLM,Tencent,https://arxiv.org/pdf/2504.01014,https://huggingface.co/papers/2504.01014,本文提出了AnimeGamer，一个基于多模态大语言模型的无限动漫生活模拟游戏。该系统允许玩家通过开放式语言指令与动漫世界中的角色互动，生成动态动画镜头和角色状态更新。与现有方法不同，AnimeGamer考虑了历史视觉上下文，确保游戏过程中的一致性和动态性。通过引入新的动作感知多模态表示，AnimeGamer能够生成高质量的视频片段，从而提升游戏体验。评估结果显示，AnimeGamer在多个方面优于现有技术，为动漫游戏的互动性和沉浸感带来了新的可能性。,20
Understanding R1-Zero-Like Training: A Critical Perspective,理解R1-Zero类训练：批判性视角,LLM,Other,https://arxiv.org/pdf/2503.20783,https://huggingface.co/papers/2503.20783,本文对R1-Zero类训练进行了深入分析，重点研究了基础模型和强化学习（RL）的相互作用。研究发现，某些基础模型（如DeepSeek-V3-Base）在预训练阶段已展现出推理能力，而Qwen2.5模型则在没有提示模板的情况下也表现出色。此外，识别出Group Relative Policy Optimization（GRPO）中的优化偏差，导致错误输出的响应长度被人为增加。为此，提出了无偏优化方法Dr. GRPO，以提高令牌效率并保持推理性能。最终，作者展示了一种简约的R1-Zero配方，使用7B基础模型在AIME 2024中达到了43.3%的准确率，创下新纪录。,18
"VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in
  One Step",VideoScene：提炼视频扩散模型以一步生成3D场景,Diffusion Model,THU,https://arxiv.org/pdf/2504.01956,https://huggingface.co/papers/2504.01956,本论文提出了VideoScene，一个新颖的方法，通过视频扩散模型在一步中生成3D场景，旨在解决从稀疏视图恢复3D场景的挑战。传统方法面临输入视图重叠不足和信息不足的问题，导致性能下降。VideoScene采用3D感知的跃迁流蒸馏策略，跳过冗余信息，并训练动态去噪策略网络，以自适应确定最佳推理时刻。实验结果表明，VideoScene在3D场景生成速度和质量上均优于现有视频扩散模型，展示了其在视频到3D应用中的潜力。,16
"DreamActor-M1: Holistic, Expressive and Robust Human Image Animation
  with Hybrid Guidance",DreamActor-M1：基于混合引导的整体、富有表现力且稳健的人物图像动画,Diffusion Model,ByteDance,https://arxiv.org/pdf/2504.01724,https://huggingface.co/papers/2504.01724,论文《DreamActor-M1》提出了一种基于扩散变换器的框架，旨在提升人像动画的整体性、表现力和鲁棒性。该方法通过混合引导信号，结合隐式面部表示、3D头部和身体骨架，实现了对面部表情和身体动作的精细控制。同时，采用渐进训练策略，使其能够适应不同姿势和图像尺度。为了确保复杂动作中的长期时间一致性，框架还整合了来自连续帧的运动模式与视觉参考。实验结果表明，DreamActor-M1在生成肖像、上半身和全身动画方面优于现有技术，展现出强大的表现力和长期一致性。,14
"ScholarCopilot: Training Large Language Models for Academic Writing with
  Accurate Citations",ScholarCopilot：为学术写作训练大型语言模型并实现准确引用,LLM,Other,https://arxiv.org/pdf/2504.00824,https://huggingface.co/papers/2504.00824,本文介绍了ScholarCopilot，一个旨在提升大型语言模型在学术写作中生成连贯文本和准确引用文献能力的统一框架。ScholarCopilot通过生成检索标记[RET]，动态决定何时检索学术参考文献，并将相关引用整合到生成过程中。该模型在500K篇arXiv论文上训练，评估结果显示其检索准确率达到40.1%，显著优于现有基线。生成质量评分为16.2/25，超越了参数量更大的模型。人类研究也证实了ScholarCopilot在引用回忆、写作效率和用户体验方面的优越性，证明了其在学术写作中的有效性。,13
"ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and
  Diffusion Refinement",ILLUME+: 通过双视觉标记化和扩散精炼照亮统一的多模态大语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2504.01934,https://huggingface.co/papers/2504.01934,论文介绍了ILLUME+，一种增强的统一多模态语言模型（MLLM），通过双重视觉标记化和扩散解码器提升深层语义理解和高保真图像生成。与现有模型相比，ILLUME+有效整合了理解、生成和编辑三大功能，采用DualViTok标记器保留细致纹理和语义对齐，支持多模态的粗到细图像表示。此外，使用扩散模型作为图像去标记器，提升生成质量和超分辨率效率。ILLUME+在多个基准测试中表现出色，为未来的多模态应用提供了可扩展和灵活的基础。,11
PaperBench: Evaluating AI's Ability to Replicate AI Research,PaperBench：评估人工智能复制人工智能研究的能力,Agent,OpenAI,https://arxiv.org/pdf/2504.01848,https://huggingface.co/papers/2504.01848,本文介绍了PaperBench，一个评估AI代理复制前沿AI研究能力的基准。AI代理需从零开始复制20篇2024年国际机器学习会议（ICML）的论文，包括理解贡献、开发代码库和成功执行实验。为实现客观评估，研究团队制定了详细的评分标准，涵盖8316个可单独评分的任务。通过使用LLM自动评分并与顶尖机器学习博士进行对比，发现当前最优秀的AI代理Claude 3.5 Sonnet的平均复制得分为21.0%，仍未超越人类基准。研究成果有助于理解AI代理的工程能力，并已开源以促进未来研究。,11
"Safeguarding Vision-Language Models: Mitigating Vulnerabilities to
  Gaussian Noise in Perturbation-based Attacks",保护视觉-语言模型：减轻对高斯噪声的脆弱性以应对扰动攻击,Multimodal LLM,Other,https://arxiv.org/pdf/2504.01308,https://huggingface.co/papers/2504.01308,本文探讨了视觉语言模型（VLMs）在处理噪声或损坏图像时的安全漏洞，尤其是对高斯噪声的敏感性。研究表明，缺乏噪声增强训练导致了严重的安全缺口。为此，提出了Robust-VLGuard，一个包含图像-文本对的多模态安全数据集，并结合噪声增强微调，显著降低攻击成功率，同时保持VLM的功能。此外，提出了DiffPure-VLM，利用扩散模型将对抗性扰动转化为高斯噪声，从而增强VLM的防御能力。实验结果表明，该方法有效缓解了不同强度的对抗扰动。相关数据集和代码已公开。,10
Articulated Kinematics Distillation from Video Diffusion Models,来自视频扩散模型的关节运动蒸馏,Diffusion Model,Other,https://arxiv.org/pdf/2504.01204,https://huggingface.co/papers/2504.01204,本文提出了“关节运动蒸馏”（AKD）框架，通过结合骨骼动画和现代生成模型的优势，实现高保真角色动画生成。AKD采用骨骼表示法，显著减少了自由度，专注于关节级控制，从而高效合成一致的运动。通过与预训练的视频扩散模型结合的得分蒸馏采样，AKD能够提取复杂的关节运动，同时保持结构完整性，克服了4D神经变形场在形状一致性方面的挑战。此外，该方法与基于物理的模拟兼容，确保了运动的物理合理性。实验表明，AKD在3D一致性和运动质量上优于现有文本到4D生成方法。,9
"Boost Your Own Human Image Generation Model via Direct Preference
  Optimization with AI Feedback",通过直接偏好优化与AI反馈提升您的人类图像生成模型,Diffusion Model,Other,https://arxiv.org/pdf/2405.20216,https://huggingface.co/papers/2405.20216,本论文提出了一种新颖的直接偏好优化（DPO）方法，旨在提升人类图像生成模型的质量。通过引入高质量真实图像作为“胜利”样本，HG-DPO（人类图像生成通过DPO）有效地指导模型生成更符合现实的人类图像，克服了传统方法中依赖生成图像的局限。该方法采用逐步学习框架，逐渐提高输出的现实性，并能够适应个性化的文本到图像任务，生成高质量且具身份特征的图像。实验结果表明，HG-DPO在解剖结构、姿势和文本图像对齐方面显著提升了人类图像生成的效果。,8
"LSNet: See Large, Focus Small",LSNet：看大，聚焦小,Other,THU,https://arxiv.org/pdf/2503.23135,https://huggingface.co/papers/2503.23135,"本论文提出了一种新型轻量级视觉网络LSNet，基于“See Large, Focus Small”的设计理念，结合大核感知和小核聚合的LS卷积方法。该方法灵感来源于人类视觉系统的动态异尺度能力，旨在提高视觉信息处理的效率和准确性。实验表明，LSNet在多种视觉任务中显著优于现有的轻量级网络，展示了在有限计算资源下实现高性能的潜力。研究结果为实际应用中的实时视觉处理提供了新的解决方案。相关代码和模型可在GitHub上获取。",3
VerifiAgent: a Unified Verification Agent in Language Model Reasoning,VerifiAgent：语言模型推理中的统一验证智能体,LLM,Other,https://arxiv.org/pdf/2504.00406,https://huggingface.co/papers/2504.00406,本文提出了VerifiAgent，一个统一的验证代理，旨在提升大型语言模型的推理可靠性。VerifiAgent结合了两级验证：元验证用于评估模型响应的完整性和一致性，工具自适应验证则根据推理类型自主选择合适的验证工具。实验结果表明，VerifiAgent在各类推理任务中优于现有的基线验证方法，并能通过验证反馈进一步提高推理准确性。此外，VerifiAgent在推理扩展方面表现出色，相较于现有模型，能以更少的样本和成本实现更好的效果。这一方法为语言模型的可靠性提供了新的解决方案。,1
"Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal
  Representations",通过多模态表示的跨模态对齐增强异常检测,Multimodal LLM,Other,https://arxiv.org/pdf/2503.18817,https://huggingface.co/papers/2503.18817,本论文提出了一种增强的跨模态对齐方法，以改善多模态表示的离群检测（OoD）性能。尽管现有的OoD检测方法多集中于单模态模型，作者指出多模态微调（MMFT）能显著提升OoD检测的效果。研究发现，传统的微调方法未能充分利用预训练知识，主要由于模态间的嵌入差距。为此，论文提出了一种新的训练目标，通过正则化图像和文本嵌入之间的距离，增强跨模态对齐。实验表明，该方法在ImageNet-1k OoD基准数据集上显著超越了现有技术，达到了最先进的OoD检测性能和领先的ID准确率。,1
