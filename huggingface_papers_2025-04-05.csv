标题,中文标题,领域分类,研究机构,PDF链接,论文链接,简明摘要,Upvote数
"Advances and Challenges in Foundation Agents: From Brain-Inspired
  Intelligence to Evolutionary, Collaborative, and Safe Systems",基础智能体的进展与挑战：从脑启发智能到进化、协作和安全系统,Agent,Other,https://arxiv.org/pdf/2504.01990,https://huggingface.co/papers/2504.01990,本文探讨了大型语言模型（LLMs）推动下的智能代理发展，提出了一种模块化的脑启发架构，整合了认知科学和计算研究的原则。文章分为四个部分：首先，分析智能代理的模块基础，映射其认知和感知功能；其次，讨论自我增强和适应性进化机制，探讨代理如何在动态环境中自我优化；第三，研究多代理系统中的协作与进化，揭示集体智能的形成；最后，强调构建安全、可靠和有益的AI系统的重要性，提出应对安全威胁和伦理对齐的策略。,109
ZClip: Adaptive Spike Mitigation for LLM Pre-Training,ZClip: 大语言模型预训练的自适应尖峰缓解,LLM,Other,https://arxiv.org/pdf/2504.02507,https://huggingface.co/papers/2504.02507,本文提出了一种名为ZClip的自适应梯度裁剪算法，旨在解决大规模语言模型（LLM）训练中的梯度不稳定和损失峰值问题。传统的梯度裁剪方法因依赖固定阈值而无法有效应对这些挑战，常导致训练效率低下和频繁的人工干预。ZClip通过动态调整裁剪阈值，基于梯度范数的统计特性，主动适应训练动态，并利用z-score异常检测识别和减轻大幅度梯度波动，从而防止恶性损失峰值的发生，同时不干扰模型收敛。这一方法显著提高了训练的稳定性和效率。,58
"Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual
  Editing",超越像素的展望：基于推理的视觉编辑基准测试,Multimodal LLM,Shanghai AI Lab,https://arxiv.org/pdf/2504.02826,https://huggingface.co/papers/2504.02826,本文介绍了RISEBench，这是首个用于评估推理驱动视觉编辑（RISE）的基准测试，旨在解决大型多模态模型在复杂指令执行、外观一致性和灵活输入格式方面的挑战。RISEBench聚焦于四种推理类型：时间、因果、空间和逻辑推理，并建立了评估框架，涵盖指令推理、外观一致性和视觉 plausibility。实验表明，尽管GPT-4o-Native在性能上优于其他模型，但在逻辑推理任务上仍显不足，显示出该领域的研究潜力。RISEBench为推理意识的视觉编辑提供了基础，未来将持续扩展和完善，以支持更全面的评估。,56
"GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image
  Generation",GPT-ImgEval：用于诊断GPT4o在图像生成中的综合基准,Multimodal LLM,Other,https://arxiv.org/pdf/2504.02782,https://huggingface.co/papers/2504.02782,本文介绍了GPT-ImgEval，一个针对OpenAI的GPT-4o模型在图像生成和编辑方面性能的综合评估基准。研究通过生成质量、编辑能力和语义合成三个维度，对GPT-4o进行了定量和定性分析。结果显示，GPT-4o在图像生成控制和输出质量上显著优于现有方法，并展示了卓越的知识推理能力。此外，文章探讨了GPT-4o的架构特征，识别了其局限性和常见的合成伪影，并进行了与Gemini 2.0 Flash的多轮图像编辑比较。研究为未来图像生成领域的研究提供了可靠的基准和见解。,40
"JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical
  Spatio-Temporal Prior Synchronization",JavisDiT：具有层次时空先验同步的联合音视频扩散变换器,Diffusion Model,Other,https://arxiv.org/pdf/2503.23377,https://huggingface.co/papers/2503.23377,"本文介绍了JavisDiT，一种新颖的联合音视频扩散变换器，旨在实现同步的音视频生成。该模型基于扩散变换器架构，能够根据开放式用户提示同时生成高质量的音频和视频。为确保最佳同步，JavisDiT引入了一种细粒度的时空对齐机制，通过层次化时空同步先验估计器提取全局和细粒度的时空信息。此外，研究团队还提出了一个新的基准数据集JavisBench，包含10,140个高质量的文本标注视频，旨在评估生成音视频对之间的同步效果。实验结果表明，JavisDiT在生成质量和同步精度上显著优于现有方法，为音视频生成任务设立了新的标准。",31
WikiVideo: Article Generation from Multiple Videos,WikiVideo: 基于多个视频生成文章,Multimodal LLM,Other,https://arxiv.org/pdf/2504.00939,https://huggingface.co/papers/2504.00939,本文提出了WikiVideo，一个自动生成高水平维基百科风格文章的基准，旨在整合来自多个视频的信息，尤其是关于自然灾害或政治选举等现实事件。现有方法主要关注文本和低级场景理解，而WikiVideo通过提供专家撰写的文章和详细注释的视频，填补了这一空白。此外，论文介绍了协作文章生成（CAG）方法，它通过与视频语言模型的互动，能够进行更高层次的推理。实验表明，CAG在信息检索和生成任务中均表现优于其他方法，展示了视频在文章生成中的潜力，并为未来研究提供了新方向。,24
"Rethinking RL Scaling for Vision Language Models: A Transparent,
  From-Scratch Framework and Comprehensive Evaluation Scheme",重新思考视觉语言模型的强化学习扩展：一个透明的从零开始框架和综合评估方案,Multimodal LLM,Other,https://arxiv.org/pdf/2504.02587,https://huggingface.co/papers/2504.02587,该论文提出了一种透明且从零开始的强化学习（RL）框架，旨在提升视觉语言模型（VLMs）的推理能力。现有的RL应用往往依赖复杂的工程框架，缺乏可重复性和标准化评估，限制了研究的可及性。作者提供了一个简化的四步流程，并提出了评估训练动态和反思行为的标准化方案。通过在视觉推理任务上的广泛实验，研究发现RL在泛化能力上优于监督微调(SFT)，并揭示了响应长度对随机种子的敏感性等重要经验结果。该框架和发现为RL在VLM研究中的更广泛应用奠定了基础。,24
Scaling Analysis of Interleaved Speech-Text Language Models,交错语音-文本语言模型的规模分析,LLM,Other,https://arxiv.org/pdf/2504.02398,https://huggingface.co/papers/2504.02398,本论文探讨了交错语音-文本语言模型（SLM）的扩展分析，回应了传统语音模型在计算和数据需求上存在的挑战。研究表明，交错SLM在计算效率上优于无文本SLM，且其扩展动态显著不同，建议在模型大小增加时更多地分配计算预算。通过对多种模型的训练和分析，结果显示，交错SLM在语音语义指标上与领先模型的表现相当，但所需的计算和数据更少。研究还探讨了合成数据和文本模型家族在提升性能中的作用。相关模型和数据已开源，供研究者使用。,22
Inference-Time Scaling for Generalist Reward Modeling,推理时扩展的通用奖励建模,LLM,Other,https://arxiv.org/pdf/2504.02495,https://huggingface.co/papers/2504.02495,本论文探讨了如何通过增强奖励建模（RM）来提升大型语言模型（LLMs）在推理时的可扩展性。研究提出了一种新的学习方法——自我原则批评调优（SPCT），结合点式生成奖励建模（GRM），实现了更灵活的输入处理和有效的推理时间扩展。通过并行采样和引入元奖励模型，研究显示SPCT显著提高了GRM的质量和可扩展性，超越了现有的多种方法和基准，且在训练时间扩展方面表现更优。尽管DeepSeek-GRM在某些任务中仍面临挑战，研究者相信未来的努力能解决这些问题。模型将被开源发布。,21
SkyReels-A2: Compose Anything in Video Diffusion Transformers,SkyReels-A2：在视频扩散变换器中合成任意内容,Diffusion Model,Other,https://arxiv.org/pdf/2504.02436,https://huggingface.co/papers/2504.02436,论文介绍了SkyReels-A2，一个可控的视频生成框架，能够根据文本提示将任意视觉元素（如角色、物体、背景）组合成合成视频，同时确保与每个元素的参考图像保持一致。该框架通过设计数据管道构建提示-参考-视频三元组，并提出了新颖的图像-文本联合嵌入模型，以实现元素间的一致性和全局协调。SkyReels-A2被认为是首个开源商业级的元素到视频生成模型，能够生成多样、高质量的视频，适用于创意应用如戏剧和虚拟电商，推动可控视频生成的边界。,18
"ShortV: Efficient Multimodal Large Language Models by Freezing Visual
  Tokens in Ineffective Layers",ShortV：通过冻结无效层中的视觉标记来实现高效的多模态大语言模型,Multimodal LLM,Other,https://arxiv.org/pdf/2504.00502,https://huggingface.co/papers/2504.00502,"本论文提出了一种名为ShortV的高效多模态大语言模型（MLLM）优化方法，通过引入层贡献（Layer Contribution, LC）度量来识别无效层，进而冻结这些层中的视觉标记更新。研究表明，许多MLLM层在处理视觉标记时贡献有限。ShortV能够在约60%的层中冻结视觉标记，显著降低计算成本，如在LLaVA-NeXT-13B模型上实现50%的FLOPs减少，同时保持优越的性能。该方法无需训练，且与现有的标记修剪技术兼容，展示了在提升效率方面的潜力。",16
"Audio-visual Controlled Video Diffusion with Masked Selective State
  Spaces Modeling for Natural Talking Head Generation",基于音视频控制的带有掩码选择状态空间建模的自然对话头生成的视频扩散,Diffusion Model,"Tencent, THU",https://arxiv.org/pdf/2504.02542,https://huggingface.co/papers/2504.02542,本文提出了一种名为ACTalker的端到端视频扩散框架，旨在生成自然的虚拟人头视频。与现有方法不同，ACTalker支持同时接受多种信号（如音频和面部运动）进行控制，克服了单一信号控制的局限性。该框架采用了并行的mamba结构，通过多个分支分别控制面部特定区域，并引入门控机制和掩码策略以避免控制冲突。实验结果表明，ACTalker能够生成自然流畅的面部视频，并有效整合多种驱动信号，具有广泛的应用潜力。,13
Efficient Model Selection for Time Series Forecasting via LLMs,通过大语言模型进行时间序列预测的高效模型选择,LLM,Other,https://arxiv.org/pdf/2504.02119,https://huggingface.co/papers/2504.02119,本论文提出了一种利用大型语言模型（LLMs）进行时间序列预测模型选择的新方法，旨在提高效率并降低计算成本。传统的模型选择依赖于广泛的性能评估和预构建的性能矩阵，耗时且资源密集。通过实验验证，作者展示了该方法在使用LLaMA、GPT和Gemini等模型时，优于传统的元学习技术和启发式基线，同时显著减少了计算开销。这一研究表明，LLMs在时间序列预测中的模型选择中具有重要的潜力，为相关领域的应用提供了更为高效的解决方案。,9
Scaling Laws in Scientific Discovery with AI and Robot Scientists,利用人工智能和机器人科学家的科学发现的规模法则,Other,Other,https://arxiv.org/pdf/2503.22444,https://huggingface.co/papers/2503.22444,本文提出了自主通用科学家（AGS）的概念，结合了人工智能和机器人技术，以自动化整个科研生命周期。当前科研面临的挑战包括手动实验耗时且资源密集，以及跨学科知识整合的困难。AGS系统能够在文献回顾、假设生成、实验和论文撰写等各个阶段动态互动，显著提高科研效率。随着这些自主系统的整合，科学发现可能遵循新的规模法则，改变知识生成和演变的方式。该研究展望了AGS在推动科学进步方面的潜力，强调其在克服现有科研障碍中的重要性。,9
Interpreting Emergent Planning in Model-Free Reinforcement Learning,无模型强化学习中的新兴规划解释,Agent,Other,https://arxiv.org/pdf/2504.01871,https://huggingface.co/papers/2504.01871,本论文首次提供了模型无关强化学习（RL）代理能够学习规划的机制性证据。通过对一种名为DRC的模型无关代理在Sokoban游戏中的分析，研究表明该代理利用学习到的概念表示来内部制定计划，预测行动对环境的长期影响并影响行动选择。研究方法包括探测与规划相关的概念、调查代理内部的计划形成以及验证这些计划对代理行为的因果影响。此外，发现这些计划的出现与代理在额外计算资源下的表现提升相吻合。论文的结果增强了对代理内部规划机制的理解，具有重要的理论和应用价值。,8
FreSca: Unveiling the Scaling Space in Diffusion Models,FreSca：揭示扩散模型中的缩放空间,Diffusion Model,Other,https://arxiv.org/pdf/2504.02154,https://huggingface.co/papers/2504.02154,本文介绍了FreSca，一种新方法，旨在深入探索扩散模型中的“缩放空间”。该研究通过傅里叶分析揭示了条件和无条件噪声预测的低频和高频成分在扩散过程中的不同演变。FreSca允许在傅里叶域中独立地对不同频率带应用引导缩放，从而显著提升现有图像编辑技术的性能，而无需重新训练。此外，FreSca在深度估计等图像理解任务中也表现出量化的改进，展示了其广泛的应用潜力和有效性。,7
"NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact
  3D Representations",NeuralGS：桥接神经场与3D高斯点云以实现紧凑的3D表示,Other,PKU,https://arxiv.org/pdf/2503.23162,https://huggingface.co/papers/2503.23162,本文提出了一种名为NeuralGS的新方法，通过结合神经场与3D高斯点云（3DGS），实现了高效的3D表示压缩。尽管3DGS在质量和渲染速度上表现优异，但其存储和传输成本高。NeuralGS通过使用多层感知器（MLP）对3D高斯属性进行编码，避免了复杂的体素结构和量化策略，显著降低了模型大小，平均减少了45倍，同时保持了视觉质量。实验结果表明，该方法在压缩性能上与现有的基于Scaffold-GS的方法相当，展示了直接利用神经场压缩3DGS的巨大潜力。,7
"GenPRM: Scaling Test-Time Compute of Process Reward Models via
  Generative Reasoning",GenPRM：通过生成推理扩展过程奖励模型的测试时间计算,LLM,"Shanghai AI Lab, THU",https://arxiv.org/pdf/2504.00891,https://huggingface.co/papers/2504.00891,本文提出了GenPRM，一种生成式过程奖励模型，旨在解决现有过程奖励模型（PRMs）在监督能力、生成能力和测试时间计算扩展方面的局限。GenPRM通过链式思维（CoT）推理和代码验证，提供每个推理步骤的判断。研究中引入了相对进展估计（RPE）和理由合成框架，以获取高质量的监督标签和推理数据。实验结果表明，GenPRM在多个任务上显著优于先前的PRMs，并在测试时间扩展中表现出色，成为政策模型优化的有效评估工具。这项研究为PRMs与LLMs中的批评模型之间架起了桥梁，开创了新的过程监督范式。,6
"Sparse Autoencoders Learn Monosemantic Features in Vision-Language
  Models",稀疏自编码器在视觉-语言模型中学习单义特征,Multimodal LLM,Other,https://arxiv.org/pdf/2504.02821,https://huggingface.co/papers/2504.02821,本论文探讨了稀疏自编码器（SAEs）在视觉-语言模型（VLMs）中的应用，特别是对CLIP模型的影响。研究表明，SAEs能够显著提高个别神经元的单义性，同时展现出与专家定义结构（如iNaturalist分类法）良好对齐的层次化表示。此外，SAEs可用于直接干预CLIP视觉编码器，进而操控多模态语言模型（如LLaVA）的输出，而无需对基础模型进行修改。这些发现强调了SAEs作为一种无监督方法在提升视觉-语言模型可解释性和可控性方面的实用性和有效性。,5
"Whisper-LM: Improving ASR Models with Language Models for Low-Resource
  Languages",Whisper-LM：通过语言模型改善低资源语言的自动语音识别模型,LLM,Other,https://arxiv.org/pdf/2503.23542,https://huggingface.co/papers/2503.23542,本研究提出了一种改进的自动语音识别（ASR）模型Whisper-LM，旨在提升低资源语言的识别性能。通过将传统和新型语言模型与经过微调的Whisper模型相结合，我们在多种数据集上进行了严格的评估，显著降低了字错误率，尤其在低资源场景中，取得了高达51%的改进。研究表明，语言模型的优化对提升各类模型的性能至关重要。此外，我们强调在报告结果时选择合适的评估参数的重要性。这一研究为实现更具包容性的ASR技术铺平了道路，增强了其对多种语言的理解能力。,4
Instruction-Guided Autoregressive Neural Network Parameter Generation,基于指令的自回归神经网络参数生成,Diffusion Model,Other,https://arxiv.org/pdf/2504.02012,https://huggingface.co/papers/2504.02012,本论文提出了IGPG（指令引导参数生成），一种自回归框架，旨在根据任务描述和架构规范生成神经网络参数。与现有基于扩散模型的方法相比，IGPG克服了大规模架构的可扩展性限制和参数生成不一致的问题。该方法结合了VQ-VAE和自回归模型，通过生成神经网络权重的令牌，确保了层间一致性，并实现了在不同模型和数据集间的高效适应。实验证明，IGPG在多个视觉数据集上表现出色，能够整合多种预训练模型，提供灵活的生成框架，并在可扩展性和效率方面优于当前最先进的方法。这使得IGPG成为快速任务特定微调和预训练权重检索的强大工具。,4
Scene-Centric Unsupervised Panoptic Segmentation,场景中心的无监督全景分割,Other,Other,https://arxiv.org/pdf/2504.01955,https://huggingface.co/papers/2504.01955,本论文提出了一种新颖的无监督全景分割方法，旨在无需人工标注数据即可对复杂场景进行语义和实例分割。与以往的研究不同，我们的方法直接基于场景中心的图像进行训练，消除了对物体中心训练数据的需求。通过结合视觉表示、深度和运动线索，我们生成了高分辨率的伪标签，并利用伪标签训练和自我训练策略，显著提高了全景分割的准确性。在Cityscapes数据集上，我们的方法在全景质量（PQ）上超越了最新的无监督全景分割技术9.4个百分点，展示了其在复杂场景理解中的有效性。,2
OpenCodeReasoning: Advancing Data Distillation for Competitive Coding,OpenCodeReasoning：推进竞争编码的数据蒸馏,LLM,Other,https://arxiv.org/pdf/2504.01943,https://huggingface.co/papers/2504.01943,本文提出了OpenCodeReasoning，一个改进的数据蒸馏方法，旨在提高编程任务中大型语言模型的能力。研究团队构建了一个高质量的监督微调（SFT）数据集，使得不同规模的模型在LiveCodeBench和CodeContests上分别达到了61.8%和24.6%的优异成绩，超越了使用强化学习训练的替代方案。论文还分析了数据来源、代码执行过滤的影响，以及指令和解决方案多样性的重要性，发现执行过滤对基准准确性产生了负面影响。最终，研究团队将开源这些数据集和蒸馏模型，以促进社区的进一步研究。,1
