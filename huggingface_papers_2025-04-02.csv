标题,中文标题,领域分类,研究机构,PDF链接,论文链接,简明摘要,Upvote数
"Any2Caption:Interpreting Any Condition to Caption for Controllable Video
  Generation",Any2Caption：在可控视频生成中解释任何条件以生成字幕,Multimodal LLM,Other,https://arxiv.org/pdf/2503.24379,https://huggingface.co/papers/2503.24379,本文提出了Any2Caption，一个新颖的可控视频生成框架，旨在改善当前视频生成领域中用户意图的准确解读。该框架通过将条件解读与视频合成步骤分离，利用多模态大语言模型（MLLM）将文本、图像、视频及其他特定提示（如区域、运动和相机姿态）转化为结构化的密集字幕，从而为视频生成提供更好的指导。此外，研究团队还推出了Any2CapIns数据集，包含337K实例和407K条件，用于指令调优。全面评估表明，该系统在可控性和视频质量方面显著优于现有模型。,41
"Exploring the Effect of Reinforcement Learning on Video Understanding:
  Insights from SEED-Bench-R1",探索强化学习对视频理解的影响：来自SEED-Bench-R1的见解,Multimodal LLM,Tencent,https://arxiv.org/pdf/2503.24376,https://huggingface.co/papers/2503.24376,本论文介绍了SEED-Bench-R1，一个用于评估多模态大语言模型（MLLMs）在视频理解任务中后训练方法的基准。通过设计复杂的现实视频和日常规划任务，SEED-Bench-R1系统地考察了模型在不同场景下的感知与推理能力。研究表明，使用强化学习（RL）进行训练的模型在数据效率和表现上优于传统的监督微调（SFT），尤其在跨环境和跨任务的场景中表现突出。然而，RL模型在逻辑推理连贯性方面存在不足，论文也指出了未来改进的方向，包括增强模型的推理能力和对噪声信号的鲁棒性。,22
"CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive
  Program Synthesis",CodeARC：评估LLM智能体在归纳程序合成中的推理能力,LLM,Other,https://arxiv.org/pdf/2503.23145,https://huggingface.co/papers/2503.23145,本论文提出了CodeARC（代码抽象与推理挑战），一个新的评估框架，用于测试大型语言模型（LLM）在归纳程序合成中的推理能力。与传统的静态评估方法不同，CodeARC允许模型通过查询隐藏的目标函数、生成候选函数并根据反馈进行自我修正，从而更真实地模拟实际编程场景。研究中构建了首个包含1114个函数的大规模基准，评估了18种模型的表现，发现o3-mini模型的成功率为52.7%。此外，通过对LLaMA-3.1-8B-Instruct进行微调，性能提升达31%。CodeARC为LLM在程序合成和归纳推理方面的评估提供了更具挑战性的测试平台。,21
JudgeLRM: Large Reasoning Models as a Judge,JudgeLRM：作为评判者的大型推理模型,LLM,Other,https://arxiv.org/pdf/2504.00050,https://huggingface.co/papers/2504.00050,本论文探讨了大型语言模型（LLMs）作为评估者的潜力，尤其是在需要复杂推理的领域中。研究发现，现有的监督微调（SFT）方法在处理高推理需求样本时表现不佳。为此，作者提出了JudgeLRM，一个基于强化学习（RL）的评估导向LLM系列，采用针对结果的奖励机制。JudgeLRM在判断任务中表现优于SFT调优模型和其他先进推理模型，尤其在深度推理任务中显著超越了GPT-4和DeepSeek-R1，展示了其在复杂评估场景中的有效性。,21
Z1: Efficient Test-time Scaling with Code,Z1：基于代码的高效测试时间扩展,LLM,THU,https://arxiv.org/pdf/2504.00810,https://huggingface.co/papers/2504.00810,本论文提出了一种高效的测试时间扩展方法，旨在提升大型语言模型（LLMs）在复杂问题解决中的表现，同时减少不必要的推理代币消耗。研究团队创建了一个名为Z1-Code-Reasoning-107K的数据集，包含简单和复杂的编码问题及其解决轨迹，并引入了“移动思维窗口”技术，去除上下文标签并限制推理代币。经过训练的Z1-7B模型能够根据问题复杂度调整推理水平，在多个推理任务中表现出与R1-Distill-Qwen-7B相当的效果，仅使用约30%的推理代币。此外，Z1-7B还展示了在更广泛推理任务中的泛化能力，为未来研究提供了重要见解。,13
"GeometryCrafter: Consistent Geometry Estimation for Open-world Videos
  with Diffusion Priors",GeometryCrafter：基于扩散先验的开放世界视频一致几何估计,Diffusion Model,"THU, Tencent",https://arxiv.org/pdf/2504.01016,https://huggingface.co/papers/2504.01016,本论文提出了GeometryCrafter，一个新颖的框架，旨在从开放世界视频中估计高保真且时间一致的点图序列，以支持3D/4D重建和相机参数估计等应用。该方法利用变分自编码器（VAE）学习与视频潜在分布无关的潜在空间，从而有效编码和解码点图。通过训练视频扩散模型，GeometryCrafter能够建模条件于输入视频的点图序列分布。实验结果表明，该方法在3D准确性、时间一致性和泛化能力上均取得了领先的表现，克服了现有方法在几何保真度方面的局限性。,13
"Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal
  LLMs on Academic Resources",Open-Qwen2VL：基于学术资源的完全开放多模态大语言模型的计算高效预训练,Multimodal LLM,ByteDance,https://arxiv.org/pdf/2504.00595,https://huggingface.co/papers/2504.00595,论文介绍了Open-Qwen2VL，一个完全开源的2B参数多模态大型语言模型，旨在提高预训练效率。该模型在仅使用442个A100-40G GPU小时的情况下，基于2900万对图像-文本数据进行预训练。通过动态图像分辨率和多模态序列打包等技术，显著提升了训练效率。研究团队采用了先进的数据过滤方法，确保了数据质量。Open-Qwen2VL在多个基准测试中超越了现有的部分开源模型，展示了其卓越的训练效率。此外，所有相关代码、数据和训练细节均已开源，推动了多模态模型的可复现性与透明性。,13
"Agent S2: A Compositional Generalist-Specialist Framework for Computer
  Use Agents",Agent S2：一种用于计算机使用智能体的组合通用专家框架,Agent,Other,https://arxiv.org/pdf/2504.00906,https://huggingface.co/papers/2504.00906,本文介绍了Agent S2，一个创新的计算机使用代理框架，旨在提高数字任务自动化的效率。Agent S2通过将认知任务分配给多种通用和专业模型，解决了当前代理在图形用户界面（GUI）元素定位、长期任务规划及性能瓶颈等方面的挑战。该框架采用新颖的混合定位技术和主动层级规划方法，能够根据动态变化的环境调整行动计划。评估结果显示，Agent S2在多个计算机使用基准测试中实现了新的最佳性能，显著超越了现有领先代理，展示了其在不同操作系统和应用中的有效泛化能力。,12
"Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for
  Large Language Models",利用推理经济：大语言模型高效推理的调查,LLM,Other,https://arxiv.org/pdf/2503.24377,https://huggingface.co/papers/2503.24377,本文综述了大语言模型（LLMs）在推理经济学方面的研究，探讨了在复杂推理任务中实现高效推理的策略。研究指出，尽管慢而深的推理（系统2）能提高任务准确性，但常伴随高计算成本，而快速直观的推理（系统1）则效率较高但表现不佳。因此，平衡推理效果与计算成本至关重要。论文分析了推理低效的原因、不同推理模式的行为特征，并提出了实现推理经济的潜在解决方案。通过提供可操作的见解和开放挑战，旨在推动该领域的研究进展，并建立了一个公共资源库以跟踪最新发展。,11
MixerMDM: Learnable Composition of Human Motion Diffusion Models,MixerMDM：可学习的人类运动扩散模型组合,Diffusion Model,Other,https://arxiv.org/pdf/2504.01019,https://huggingface.co/papers/2504.01019,本文提出了MixerMDM，一种首个可学习的模型组合技术，用于结合预训练的文本条件人类动作扩散模型。传统的合并策略未能考虑不同生成模型的特性及具体文本描述的影响，而MixerMDM采用动态混合策略，通过对抗训练学习优化每个模型的去噪过程。该方法能够实现对单人和多人动作的精细控制，同时提升人际互动的生成质量。此外，本文还提出了一种新评估技术，首次量化生成动作与条件之间的对齐程度，验证了MixerMDM在动态混合过程中的适应能力。,11
"OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming
  Video Contexts",OmniMMI: 一种综合性的多模态交互基准在流媒体视频上下文中,Multimodal LLM,PKU,https://arxiv.org/pdf/2503.22952,https://huggingface.co/papers/2503.22952,"本论文提出了OmniMMI，一个针对流媒体视频环境的全面多模态互动基准，旨在评估新一代多模态语言模型（OmniLLMs）的实时互动能力。OmniMMI包含超过1,121个视频和2,290个问题，聚焦于流媒体视频理解和主动推理这两个关键挑战，涵盖六个不同的子任务。作者还提出了一种新框架——多模态多路复用建模（M4），以提高推理效率并支持实时生成。实验结果表明，现有的多模态语言模型在互动流媒体理解方面存在不足，而M4在处理主动任务和多轮查询时表现出显著提升。这项研究为未来多模态互动系统的设计和评估提供了重要参考。",10
Command A: An Enterprise-Ready Large Language Model,Command A：一款企业级大型语言模型,LLM,Other,https://arxiv.org/pdf/2504.00698,https://huggingface.co/papers/2504.00698,本文介绍了Command A的开发，这是一个专为企业应用设计的大型语言模型，具备23种语言的支持和卓越的性能。Command A采用创新的混合架构，优化了效率，并提供最佳的检索增强生成（RAG）能力，能够自动化复杂的商业流程。该模型通过去中心化的训练方法和自我优化算法实现高效性能。此外，论文还介绍了与Command A相似的Command R7B模型，并发布了两者的权重以供研究。通过全面的评估，结果显示Command A在多个企业相关任务和公共基准测试中表现出色。,9
"Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on
  Elementary School-Level Reasoning Problems?",背诵与推理：前沿语言模型如何在小学水平的推理问题上失败？,LLM,ByteDance,https://arxiv.org/pdf/2504.00509,https://huggingface.co/papers/2504.00509,本论文探讨了当前先进的大型语言模型（LLMs）在解决小学水平推理问题时的表现，提出了一个名为RoR-Bench的新基准，以检测模型的“背诵”行为。研究发现，尽管这些模型在一般情况下表现出色，但当问题条件稍作调整时，它们的表现会显著下降，最高可达60%的失误率。这一结果揭示了LLMs在推理能力上的局限性，呼吁研究者重新评估这些模型的真正智能水平，强调了对其能力的深入理解的重要性。,9
"AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models
  with Unsupervised Coefficient Optimization",AdaMMS：针对异构多模态大语言模型的模型合并与无监督系数优化,Multimodal LLM,"THU, PKU, Alibaba",https://arxiv.org/pdf/2503.23733,https://huggingface.co/papers/2503.23733,本文提出了一种新颖的模型合并方法AdaMMS，专为异构多模态大语言模型（MLLMs）设计。与以往主要针对同构模型的合并方法不同，AdaMMS能够有效处理模型架构差异和参数空间不对称的问题。该方法通过三个步骤实现：首先设计模型之间的映射函数以适应不同架构，其次通过线性插值调整模型权重，最后提出了一种无监督的超参数选择方法。实验结果表明，AdaMMS在多种视觉-语言基准测试中优于现有的模型合并技术，展示了其在无标签数据情况下的有效性和优势。,7
"When To Solve, When To Verify: Compute-Optimal Problem Solving and
  Generative Verification for LLM Reasoning",何时求解，何时验证：大语言模型推理的计算最优问题解决与生成验证,LLM,Google,https://arxiv.org/pdf/2504.01005,https://huggingface.co/papers/2504.01005,本论文探讨了在有限推理预算下，大型语言模型（LLMs）在数学问题解决中的计算优化策略。研究比较了两种方法：自一致性（SC）和生成奖励模型（GenRM）。结果显示，在大多数实际推理预算下，SC比GenRM更具计算效率，尤其在低预算时，SC所需计算量可低至GenRM的八分之一。论文还提出了GenRM的推理扩展规律，强调在计算最优化时，更应优先扩大解的生成而非验证的数量。研究为优化测试时的计算资源分配提供了实用指导。,7
Scaling Language-Free Visual Representation Learning,扩展无语言视觉表征学习,Multimodal LLM,Meta,https://arxiv.org/pdf/2504.01017,https://huggingface.co/papers/2504.01017,该论文探讨了视觉自监督学习（SSL）与语言对比图像预训练（CLIP）在多模态任务（如视觉问答）中的表现差异。研究发现，当在相同数据集（MetaCLIP）上训练时，视觉SSL模型在数据和模型容量的扩展性上优于CLIP，并且其性能在参数规模达到70亿后仍未饱和。视觉SSL方法在多个视觉问答和经典视觉基准测试中达到了与CLIP相当的表现。这一发现表明，纯视觉SSL在规模上能够与语言监督的视觉预训练相匹配，为视觉中心的表示学习开辟了新机会。,7
Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features,通过修剪交叉注意的视觉特征实现高效的LLaMA-3.2-Vision,Multimodal LLM,Other,https://arxiv.org/pdf/2504.00557,https://huggingface.co/papers/2504.00557,本文提出了一种名为Trimmed LLaMA的高效视觉语言模型，旨在通过减少交叉注意力层中的视觉特征来降低推理成本。研究发现，交叉注意力模型中的图像令牌的关键值缓存（KV cache）需求显著高于自注意力模型中的文本令牌，从而造成计算瓶颈。通过利用交叉注意力图的稀疏特性，作者有效地修剪冗余的视觉特征，实现了50%的视觉特征减少，显著降低了推理延迟和内存使用，同时保持了在基准测试中的表现一致性。这一方法无需额外训练，展现了在高分辨率环境中的应用潜力。,6
Multi-Token Attention,多标记注意力,LLM,Meta,https://arxiv.org/pdf/2504.00927,https://huggingface.co/papers/2504.00927,"本文提出了一种新的注意力机制——多令牌注意力（Multi-Token Attention, MTA），旨在克服传统单令牌注意力在处理上下文信息时的局限性。MTA通过同时考虑多个查询和键向量，利用卷积操作增强注意力权重的计算，使得相邻的查询和键能相互影响，从而实现更精确的上下文定位。实验结果表明，MTA在多个基准测试中表现优于传统的Transformer模型，特别是在需要从长上下文中检索信息的任务中，展现出更强的能力。该方法为大型语言模型提供了更丰富的信息处理能力。",6
Towards Trustworthy GUI Agents: A Survey,迈向可信赖的GUI智能体：一项调查,Agent,"Tencent, Amazon",https://arxiv.org/pdf/2503.23434,https://huggingface.co/papers/2503.23434,本论文对GUI代理的可信性进行了全面调查，重点分析了安全漏洞、动态环境中的可靠性、透明性与可解释性、伦理问题及评估方法等五个关键维度。随着这些代理在网络自动化、移动导航和软件测试等领域的广泛应用，其自主性带来了安全、隐私和安全性等重要担忧。论文还识别了主要挑战，如对抗攻击的脆弱性和评估基准的缺乏，强调了制定全面的缓解策略和建立安全标准的重要性。此研究为推动可信的GUI代理提供了系统性的理解和未来研究的基础。,6
"Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies
  Ahead",推理时间扩展在复杂任务中的应用：我们所处的位置与未来展望,LLM,Microsoft,https://arxiv.org/pdf/2504.00294,https://huggingface.co/papers/2504.00294,本文探讨了推理时间扩展对大型语言模型（LLMs）在复杂任务中的推理能力的影响，尤其是在逐步问题解决方面的应用。研究比较了九种先进模型在八个挑战性任务（如数学推理、日历规划和导航等）上的表现，评估了常规模型与针对推理时间扩展进行微调的模型的性能。结果表明，推理时间扩展的优势因任务而异，并且在问题复杂性增加时效果减弱。此外，简单增加生成的标记数量并不总能提高准确性。研究还发现，使用完美验证器或强反馈时，所有模型在推理能力上均有显著提升，显示出未来改进的潜力。,5
"Discovering Knowledge Deficiencies of Language Models on Massive
  Knowledge Base",发现大语言模型在大规模知识库上的知识缺陷,LLM,Amazon,https://arxiv.org/pdf/2503.23361,https://huggingface.co/papers/2503.23361,本论文提出了一种名为随机错误上升（SEA）的新框架，用于高效发现闭合权重语言模型（LLMs）中的知识缺陷。SEA通过将错误发现视为一种随机优化过程，利用先前观察到的错误的语义相似性，逐步检索新的高错误候选项。该方法在文档和段落层面上进行分层检索，并构建关系有向无环图以识别系统性失败模式。实验证明，SEA比现有方法发现了更多的知识错误，同时显著降低了每个错误的成本。这一研究强调了未来LLM开发中改善数据覆盖和针对性微调的必要性。,4
"m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning
  with Large Language Models",m1：释放测试时间缩放在大型语言模型医疗推理中的潜力,LLM,Amazon,https://arxiv.org/pdf/2504.00869,https://huggingface.co/papers/2504.00869,本论文探讨了测试时间扩展（test-time scaling）在医学推理中的应用，提出了一种名为m1的方法，以提升大型语言模型的医学推理能力。研究表明，通过增加“思考”令牌预算，m1在多项医学任务中实现了显著的性能提升，使得小于10B参数的模型达到了新的最优表现，而32B模型的表现与70B规模的医学模型相当。论文还发现，约4K的最佳推理令牌预算是关键，过度思考可能导致性能下降。此外，提升医学知识的质量和数量是进一步提高性能的关键。这些发现强调了医学推理与数学推理之间的根本差异。,3
Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs,Chapter-Llama：基于大语言模型的高效长视频章节划分,Multimodal LLM,Google,https://arxiv.org/pdf/2504.00072,https://huggingface.co/papers/2504.00072,本论文提出了“Chapter-Llama”框架，旨在高效地为长达一小时的视频进行章节划分和标题生成。通过利用预训练的大型语言模型（LLM），该方法将语音转录文本和视频帧描述作为输入，并采用基于语音内容的轻量级帧选择策略，以避免对所有帧进行逐一标注。实验结果表明，该方法在VidChapters-7M基准测试中显著提高了章节检测的性能（F1分数从26.7提升至45.3），展示了其在长视频内容导航和检索中的潜力。研究团队还公开了相关代码和模型，以促进后续研究。,3
"ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via
  Residual Learning",ManipTrans：通过残差学习实现高效灵巧双手操控技能转移,Embodied AI,"THU, PKU",https://arxiv.org/pdf/2503.21860,https://huggingface.co/papers/2503.21860,"本文提出了一种名为ManipTrans的新方法，用于高效地将人类双手的灵巧操作技能转移到机器人手上。ManipTrans采用两阶段策略：首先，通过通用轨迹模仿器预训练手部运动，然后在交互约束下微调特定的残差模块，从而实现复杂双手任务的高效学习和准确执行。实验结果表明，该方法在成功率、精确度和效率上均优于现有技术。此外，研究团队还创建了DexManipNet，一个包含3,300个机器人操作实例的大规模数据集，支持进一步的策略训练和实际应用。",2
"DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D
  Gaussian Splatting",DiET-GS：扩散先验与事件流辅助的运动去模糊3D高斯点云,Diffusion Model,Other,https://arxiv.org/pdf/2503.24210,https://huggingface.co/papers/2503.24210,本论文提出了DiET-GS，一个结合扩散先验和事件流的运动去模糊3D高斯点云重建框架。该方法旨在从模糊的多视图图像中重建清晰的3D表示，克服了现有技术在颜色恢复和细节保留方面的不足。通过两阶段训练策略，DiET-GS有效利用无模糊的事件流和扩散先验，采用事件双重积分约束3D点云，以实现准确的颜色和清晰的细节。此外，论文还提出了一种简单的技术来进一步增强边缘细节。实验结果表明，DiET-GS在合成和真实数据上均显著提升了新视角合成的质量，优于现有基线方法。,2
"Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards
  for Reasoning-Enhanced Text-to-SQL",Reasoning-SQL：针对增强推理的文本到SQL的强化学习与SQL定制部分奖励,LLM,DeepMind,https://arxiv.org/pdf/2503.23157,https://huggingface.co/papers/2503.23157,本论文提出了一种名为Reasoning-SQL的新方法，旨在通过特定的部分奖励机制提升文本到SQL的生成能力。该方法针对文本到SQL任务中的推理挑战，设计了包括模式链接、AI反馈、n-gram相似度和语法检查在内的奖励体系，以解决强化学习中的奖励稀疏问题。通过群体相对策略优化，模型能够发展出必要的内在推理技能。实验结果表明，使用该方法进行强化学习训练的模型在准确性和泛化能力上均优于传统的监督微调方法，尤其是14B参数模型在BIRD基准测试中显著超越了更大规模的专有模型。这一研究展示了部分奖励机制在增强文本到SQL任务中的有效性。,1
"MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote
  Sensing",MB-ORES：用于遥感视觉定位的多分支对象推理器,Other,Other,https://arxiv.org/pdf/2503.24219,https://huggingface.co/papers/2503.24219,本文提出了一种名为MB-ORES的统一框架，旨在将物体检测（OD）与视觉定位（VG）有效结合，专注于遥感图像处理。通过对开放集物体检测器进行微调，模型在第一阶段构建了图像的图形表示，包含物体查询、类别嵌入和提议位置。随后，采用多分支网络整合空间、视觉和类别特征生成任务感知的提议，并通过对象推理网络进行概率分配，最终实现对所指物体的定位。该模型在OPT-RSVG和DIOR-RSVG数据集上表现优异，超越了现有先进方法，同时保持了传统物体检测的能力。,0
